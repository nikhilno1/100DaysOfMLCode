{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Submission DNST_CIFAR10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/nikhilno1/100DaysOfMLCode/blob/master/DNST_CIFAR10.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "K70hAckqg0EA",
        "colab_type": "code",
        "outputId": "10184c44-483b-4b1d-a58d-7acbcd3e1778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "wVIx_KIigxPV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from scipy.misc import toimage\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UNHw6luQg3gc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this part will prevent tensorflow to allocate all the available GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mp9ytfaZ_N6P",
        "colab_type": "code",
        "outputId": "e8a71322-0d98-4a49-ec5c-73ddc7ae82f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -rf clr_callback.py*\n",
        "!wget https://github.com/bckenstler/CLR/raw/master/clr_callback.py\n",
        "from clr_callback import *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-19 08:17:40--  https://github.com/bckenstler/CLR/raw/master/clr_callback.py\n",
            "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/bckenstler/CLR/master/clr_callback.py [following]\n",
            "--2018-10-19 08:17:40--  https://raw.githubusercontent.com/bckenstler/CLR/master/clr_callback.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5326 (5.2K) [text/plain]\n",
            "Saving to: ‘clr_callback.py’\n",
            "\n",
            "\rclr_callback.py       0%[                    ]       0  --.-KB/s               \rclr_callback.py     100%[===================>]   5.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2018-10-19 08:17:40 (48.8 MB/s) - ‘clr_callback.py’ saved [5326/5326]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ngfZDqIn169P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3UF_GpLv18dK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(test_x, test_y, model):\n",
        "    result = model.predict(test_x)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dsO_yGxcg5D8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 250\n",
        "l = 12\n",
        "nb_filter = 24\n",
        "compression = 0.9\n",
        "dropout_rate = 0.0\n",
        "growth_rate = 12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Md0-RFxigndF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lr_schedule(epoch):\n",
        "    #lrate = 0.001\n",
        "    lrate = 0.01\n",
        "    if epoch > 75:\n",
        "        #lrate = 0.0005\n",
        "        lrate = 0.001\n",
        "    elif epoch > 150:\n",
        "        #lrate = 0.0003\n",
        "        lrate = 0.0001\n",
        "    return lrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mB7o3zu1g6eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoding \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ee-sge5Kg7vr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dense Block\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression, growth_rate, nb_filter\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(growth_rate, (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        nb_filter += growth_rate\n",
        "        temp = concat\n",
        "        \n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OOP6IPsGhBwb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter * compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0RaKFpubhDIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def output_layer(input):\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    #AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    GlobalAvg = GlobalAveragePooling2D()(relu)\n",
        "    #flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(GlobalAvg)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "anPCpQWhhGb7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#num_filter = 24\n",
        "#dropout_rate = 0.2\n",
        "#l = 12\n",
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = Conv2D(nb_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, nb_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, nb_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, nb_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, nb_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, nb_filter, dropout_rate)\n",
        "#Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "output = output_layer(Third_Block)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1kFh7pdxhNtT",
        "colab_type": "code",
        "outputId": "46cf6bfa-76c1-464a-9d7f-789e212bf387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6120
        }
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 24)   648         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 24)   96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 24)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 12)   2592        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 36)   0           conv2d_1[0][0]                   \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 36)   144         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 36)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 12)   3888        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 48)   0           concatenate_1[0][0]              \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 48)   192         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 48)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 12)   5184        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 60)   0           concatenate_2[0][0]              \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 60)   240         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 60)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 12)   6480        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 72)   0           concatenate_3[0][0]              \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 72)   288         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 72)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 12)   7776        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 84)   0           concatenate_4[0][0]              \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 84)   336         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 84)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 12)   9072        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 96)   0           concatenate_5[0][0]              \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 96)   384         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 96)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 12)   10368       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 108)  0           concatenate_6[0][0]              \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 108)  432         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 108)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 12)   11664       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 120)  0           concatenate_7[0][0]              \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 120)  480         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 120)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 12)   12960       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 132)  0           concatenate_8[0][0]              \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 132)  528         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 132)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 12)   14256       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 144)  0           concatenate_9[0][0]              \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 144)  576         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 144)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 12)   15552       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 156)  0           concatenate_10[0][0]             \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 156)  624         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 156)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 12)   16848       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 168)  0           concatenate_11[0][0]             \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 168)  672         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 168)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 151)  25368       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 151)  0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 151)  604         average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 151)  0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 12)   16308       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 163)  0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 163)  652         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 163)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 12)   17604       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 175)  0           concatenate_13[0][0]             \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 175)  700         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 175)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 12)   18900       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 187)  0           concatenate_14[0][0]             \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 187)  748         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 187)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 12)   20196       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 199)  0           concatenate_15[0][0]             \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 199)  796         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 199)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 12)   21492       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 211)  0           concatenate_16[0][0]             \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 211)  844         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 211)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 12)   22788       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 223)  0           concatenate_17[0][0]             \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 223)  892         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 223)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 12)   24084       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 235)  0           concatenate_18[0][0]             \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 235)  940         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 235)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 12)   25380       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 247)  0           concatenate_19[0][0]             \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 247)  988         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 247)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 12)   26676       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 259)  0           concatenate_20[0][0]             \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 259)  1036        concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 259)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 12)   27972       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 271)  0           concatenate_21[0][0]             \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 271)  1084        concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 271)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 12)   29268       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 283)  0           concatenate_22[0][0]             \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 283)  1132        concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 283)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 12)   30564       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 295)  0           concatenate_23[0][0]             \n",
            "                                                                 conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 295)  1180        concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 295)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 280)  82600       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 280)    0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 280)    1120        average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 280)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 12)     30240       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 292)    0           average_pooling2d_2[0][0]        \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 292)    1168        concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 292)    0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 12)     31536       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 304)    0           concatenate_25[0][0]             \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 304)    1216        concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 304)    0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 12)     32832       activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 316)    0           concatenate_26[0][0]             \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 316)    1264        concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 316)    0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 12)     34128       activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 328)    0           concatenate_27[0][0]             \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 328)    1312        concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 328)    0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 12)     35424       activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 340)    0           concatenate_28[0][0]             \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 340)    1360        concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 340)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 12)     36720       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 352)    0           concatenate_29[0][0]             \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 352)    1408        concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 352)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 12)     38016       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 364)    0           concatenate_30[0][0]             \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 364)    1456        concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 364)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 12)     39312       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 376)    0           concatenate_31[0][0]             \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 376)    1504        concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 376)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 12)     40608       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 388)    0           concatenate_32[0][0]             \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 388)    1552        concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 388)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 12)     41904       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 400)    0           concatenate_33[0][0]             \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 400)    1600        concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 400)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 12)     43200       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 412)    0           concatenate_34[0][0]             \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 412)    1648        concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 412)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 12)     44496       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 424)    0           concatenate_35[0][0]             \n",
            "                                                                 conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 424)    1696        concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 424)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 424)          0           activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           4250        global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 994,046\n",
            "Trainable params: 976,600\n",
            "Non-trainable params: 17,446\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bZ5xJwV5L-rq",
        "colab_type": "code",
        "outputId": "be46f0bc-571b-41ce-ede2-3eebc26af6bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#model.load_weights(\"/content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_aug.best.h5\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "How_jcUCWhlC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NE92rHJDNsv8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sVdDHn4AZSoX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lr_reducer      = ReduceLROnPlateau(monitor='val_acc', factor=np.sqrt(0.1),\n",
        "                                    cooldown=0, patience=5, min_lr=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b4XOsW3ahSkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sP5o8hxODJVK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "crhGk7kEhXAz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run_type = \"RunX_IMG_AUG_LR_SCH\"\n",
        "# epoch_set = 10\n",
        "# for ep in range(1,26):\n",
        "#   print('Running iteration: ' + str(ep))\n",
        "#   #model.fit(x_train, y_train,\n",
        "#   #                  batch_size=batch_size,\n",
        "#   #                  epochs=epoch_set,\n",
        "#   #                  verbose=1,\n",
        "#   #                  validation_data=(x_test, y_test))\n",
        "#   #model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "#   #                  steps_per_epoch=x_train.shape[0] // batch_size,epochs=epoch_set,\\\n",
        "#   #                  verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])  \n",
        "#   path = \"/content/gdrive/My Drive/Model Weights/DNST_CIFAR10/DNST_model_\" + run_type + \"_\" + str((ep * epoch_set))\n",
        "#   model.save_weights(path)\n",
        "#   model_json = model.to_json()\n",
        "#   json_path = path + \".json\"\n",
        "#   with open(json_path, 'w') as json_file:\n",
        "#     json_file.write(model_json)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBCgLZtupeJm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# checkpoint\n",
        "filepath=\"/content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "#clr_triangular = CyclicLR(mode='triangular2')\n",
        "#clr_triangular = CyclicLR(base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular2')\n",
        "callbacks_list = [checkpoint, lr_reducer]\n",
        "#callbacks_list = [checkpoint, LearningRateScheduler(lr_schedule)]\n",
        "#callbacks_list = [checkpoint, clr_triangular]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1oTp_jR09VG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#epochs=155"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4_dcujlA56p",
        "colab_type": "code",
        "outputId": "4507da97-0682-425c-e36f-898fd00b34f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4916
        }
      },
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\\\n",
        "                    verbose=1, validation_data=(x_test,y_test),\\\n",
        "                    callbacks=callbacks_list) \n",
        "end = time.time()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "Epoch 1/250\n",
            "781/781 [==============================] - 283s 362ms/step - loss: 1.4007 - acc: 0.4924 - val_loss: 1.5455 - val_acc: 0.4917\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.49170, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 2/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.9953 - acc: 0.6457 - val_loss: 1.1008 - val_acc: 0.6185\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.49170 to 0.61850, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 3/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.8350 - acc: 0.7047 - val_loss: 2.5236 - val_acc: 0.4152\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.61850\n",
            "Epoch 4/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.7295 - acc: 0.7467 - val_loss: 1.1593 - val_acc: 0.6473\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.61850 to 0.64730, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 5/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.6486 - acc: 0.7756 - val_loss: 1.0186 - val_acc: 0.6819\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.64730 to 0.68190, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 6/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.5954 - acc: 0.7929 - val_loss: 1.0501 - val_acc: 0.6813\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.68190\n",
            "Epoch 7/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.5470 - acc: 0.8114 - val_loss: 0.9239 - val_acc: 0.7140\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.68190 to 0.71400, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 8/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.5142 - acc: 0.8217 - val_loss: 0.7645 - val_acc: 0.7626\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.71400 to 0.76260, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 9/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.4826 - acc: 0.8331 - val_loss: 0.8599 - val_acc: 0.7387\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.76260\n",
            "Epoch 10/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.4562 - acc: 0.8443 - val_loss: 0.7889 - val_acc: 0.7530\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.76260\n",
            "Epoch 11/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.4341 - acc: 0.8513 - val_loss: 1.0393 - val_acc: 0.7139\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.76260\n",
            "Epoch 12/250\n",
            "781/781 [==============================] - 272s 348ms/step - loss: 0.4146 - acc: 0.8581 - val_loss: 0.8427 - val_acc: 0.7580\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.76260\n",
            "Epoch 13/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.3938 - acc: 0.8641 - val_loss: 0.8913 - val_acc: 0.7597\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.76260\n",
            "Epoch 14/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.3181 - acc: 0.8915 - val_loss: 0.4480 - val_acc: 0.8481\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.76260 to 0.84810, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 15/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.2948 - acc: 0.8971 - val_loss: 0.4115 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.84810 to 0.85940, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 16/250\n",
            "781/781 [==============================] - 271s 348ms/step - loss: 0.2855 - acc: 0.9022 - val_loss: 0.4676 - val_acc: 0.8501\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.85940\n",
            "Epoch 17/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.2777 - acc: 0.9040 - val_loss: 0.4480 - val_acc: 0.8551\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.85940\n",
            "Epoch 18/250\n",
            "781/781 [==============================] - 272s 348ms/step - loss: 0.2724 - acc: 0.9056 - val_loss: 0.5172 - val_acc: 0.8375\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.85940\n",
            "Epoch 19/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.2653 - acc: 0.9084 - val_loss: 0.4158 - val_acc: 0.8645\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.85940 to 0.86450, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 20/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.2633 - acc: 0.9090 - val_loss: 0.5240 - val_acc: 0.8362\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.86450\n",
            "Epoch 21/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.2517 - acc: 0.9136 - val_loss: 0.4603 - val_acc: 0.8515\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.86450\n",
            "Epoch 22/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.2492 - acc: 0.9146 - val_loss: 0.5037 - val_acc: 0.8428\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.86450\n",
            "Epoch 23/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.2422 - acc: 0.9158 - val_loss: 0.4843 - val_acc: 0.8437\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.86450\n",
            "Epoch 24/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.2377 - acc: 0.9158 - val_loss: 0.4066 - val_acc: 0.8684\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.86450 to 0.86840, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 25/250\n",
            "781/781 [==============================] - 271s 348ms/step - loss: 0.2371 - acc: 0.9172 - val_loss: 0.4337 - val_acc: 0.8648\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.86840\n",
            "Epoch 26/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.2260 - acc: 0.9210 - val_loss: 0.4140 - val_acc: 0.8656\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.86840\n",
            "Epoch 27/250\n",
            "781/781 [==============================] - 271s 348ms/step - loss: 0.2260 - acc: 0.9211 - val_loss: 0.4191 - val_acc: 0.8689\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.86840 to 0.86890, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 28/250\n",
            "781/781 [==============================] - 271s 346ms/step - loss: 0.2192 - acc: 0.9235 - val_loss: 0.4119 - val_acc: 0.8678\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.86890\n",
            "Epoch 29/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.2191 - acc: 0.9241 - val_loss: 0.5019 - val_acc: 0.8463\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.86890\n",
            "Epoch 30/250\n",
            "781/781 [==============================] - 272s 348ms/step - loss: 0.2169 - acc: 0.9245 - val_loss: 0.4587 - val_acc: 0.8634\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.86890\n",
            "Epoch 31/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.2083 - acc: 0.9279 - val_loss: 0.6052 - val_acc: 0.8304\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.86890\n",
            "Epoch 32/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.2080 - acc: 0.9270 - val_loss: 0.4330 - val_acc: 0.8653\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.86890\n",
            "Epoch 33/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1708 - acc: 0.9424 - val_loss: 0.3527 - val_acc: 0.8856\n",
            "\n",
            "Epoch 00033: val_acc improved from 0.86890 to 0.88560, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 34/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1655 - acc: 0.9433 - val_loss: 0.3601 - val_acc: 0.8877\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.88560 to 0.88770, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 35/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1595 - acc: 0.9464 - val_loss: 0.4142 - val_acc: 0.8730\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.88770\n",
            "Epoch 36/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1606 - acc: 0.9456 - val_loss: 0.3453 - val_acc: 0.8902\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.88770 to 0.89020, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 37/250\n",
            "781/781 [==============================] - 272s 348ms/step - loss: 0.1544 - acc: 0.9463 - val_loss: 0.3430 - val_acc: 0.8888\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.89020\n",
            "Epoch 38/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1526 - acc: 0.9480 - val_loss: 0.3720 - val_acc: 0.8839\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.89020\n",
            "Epoch 39/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1491 - acc: 0.9494 - val_loss: 0.3453 - val_acc: 0.8867\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.89020\n",
            "Epoch 40/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1488 - acc: 0.9484 - val_loss: 0.3475 - val_acc: 0.8895\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.89020\n",
            "Epoch 41/250\n",
            "781/781 [==============================] - 272s 348ms/step - loss: 0.1500 - acc: 0.9481 - val_loss: 0.3670 - val_acc: 0.8860\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.89020\n",
            "Epoch 42/250\n",
            "781/781 [==============================] - 271s 346ms/step - loss: 0.1392 - acc: 0.9527 - val_loss: 0.3185 - val_acc: 0.8980\n",
            "\n",
            "Epoch 00042: val_acc improved from 0.89020 to 0.89800, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 43/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1359 - acc: 0.9536 - val_loss: 0.3197 - val_acc: 0.8976\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.89800\n",
            "Epoch 44/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1336 - acc: 0.9542 - val_loss: 0.3371 - val_acc: 0.8936\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.89800\n",
            "Epoch 45/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1330 - acc: 0.9530 - val_loss: 0.3276 - val_acc: 0.8949\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.89800\n",
            "Epoch 46/250\n",
            "781/781 [==============================] - 271s 346ms/step - loss: 0.1331 - acc: 0.9550 - val_loss: 0.3166 - val_acc: 0.8987\n",
            "\n",
            "Epoch 00046: val_acc improved from 0.89800 to 0.89870, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch.best.h5\n",
            "Epoch 47/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1289 - acc: 0.9573 - val_loss: 0.3280 - val_acc: 0.8965\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.89870\n",
            "Epoch 48/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1315 - acc: 0.9549 - val_loss: 0.3319 - val_acc: 0.8936\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.89870\n",
            "Epoch 49/250\n",
            "781/781 [==============================] - 271s 346ms/step - loss: 0.1304 - acc: 0.9547 - val_loss: 0.3309 - val_acc: 0.8939\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.89870\n",
            "Epoch 50/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1275 - acc: 0.9567 - val_loss: 0.3280 - val_acc: 0.8953\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.89870\n",
            "Epoch 51/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1249 - acc: 0.9576 - val_loss: 0.3224 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.89870\n",
            "Epoch 52/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1238 - acc: 0.9579 - val_loss: 0.3280 - val_acc: 0.8972\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.89870\n",
            "Epoch 53/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1228 - acc: 0.9577 - val_loss: 0.3268 - val_acc: 0.8974\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.89870\n",
            "Epoch 54/250\n",
            "781/781 [==============================] - 272s 348ms/step - loss: 0.1249 - acc: 0.9577 - val_loss: 0.3264 - val_acc: 0.8973\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.89870\n",
            "Epoch 55/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1252 - acc: 0.9578 - val_loss: 0.3265 - val_acc: 0.8977\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.89870\n",
            "Epoch 56/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1220 - acc: 0.9587 - val_loss: 0.3275 - val_acc: 0.8972\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.89870\n",
            "Epoch 57/250\n",
            "781/781 [==============================] - 270s 345ms/step - loss: 0.1227 - acc: 0.9579 - val_loss: 0.3265 - val_acc: 0.8969\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.89870\n",
            "Epoch 58/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1205 - acc: 0.9588 - val_loss: 0.3254 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.89870\n",
            "Epoch 59/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1204 - acc: 0.9597 - val_loss: 0.3252 - val_acc: 0.8971\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.89870\n",
            "Epoch 60/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1218 - acc: 0.9585 - val_loss: 0.3273 - val_acc: 0.8971\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.89870\n",
            "Epoch 61/250\n",
            "781/781 [==============================] - 272s 348ms/step - loss: 0.1222 - acc: 0.9579 - val_loss: 0.3272 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.89870\n",
            "Epoch 62/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1206 - acc: 0.9582 - val_loss: 0.3242 - val_acc: 0.8970\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.89870\n",
            "Epoch 63/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1190 - acc: 0.9599 - val_loss: 0.3268 - val_acc: 0.8971\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.89870\n",
            "Epoch 64/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1207 - acc: 0.9590 - val_loss: 0.3278 - val_acc: 0.8974\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.89870\n",
            "Epoch 65/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1213 - acc: 0.9591 - val_loss: 0.3247 - val_acc: 0.8981\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.89870\n",
            "Epoch 66/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1189 - acc: 0.9589 - val_loss: 0.3253 - val_acc: 0.8981\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.89870\n",
            "Epoch 67/250\n",
            "781/781 [==============================] - 270s 346ms/step - loss: 0.1202 - acc: 0.9589 - val_loss: 0.3257 - val_acc: 0.8979\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.89870\n",
            "Epoch 68/250\n",
            "781/781 [==============================] - 272s 348ms/step - loss: 0.1180 - acc: 0.9598 - val_loss: 0.3258 - val_acc: 0.8978\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.89870\n",
            "Epoch 69/250\n",
            "781/781 [==============================] - 269s 345ms/step - loss: 0.1180 - acc: 0.9605 - val_loss: 0.3226 - val_acc: 0.8983\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.89870\n",
            "Epoch 70/250\n",
            "781/781 [==============================] - 271s 347ms/step - loss: 0.1215 - acc: 0.9575 - val_loss: 0.3249 - val_acc: 0.8985\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.89870\n",
            "Epoch 71/250\n",
            "781/781 [==============================] - 269s 345ms/step - loss: 0.1205 - acc: 0.9588 - val_loss: 0.3265 - val_acc: 0.8984\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.89870\n",
            "Epoch 72/250\n",
            "152/781 [====>.........................] - ETA: 3:22 - loss: 0.1225 - acc: 0.9586"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yzvKjFGh9sog",
        "colab_type": "code",
        "outputId": "2223f7a8-c480-462d-f57a-7dade7aea33f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "cell_type": "code",
      "source": [
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "print(\"Model took %0.2f seconds to train\" % (end - start))\n",
        "# compute test accuracy\n",
        "print(\"Accuracy on test data is: %0.2f\" % accuracy(x_test, y_test, model))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3wAAAFMCAYAAACQ8b6mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0aed778dd8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Model took 1137.36 seconds to train\n",
            "Accuracy on test data is: 38.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZcWydmIVhZGr",
        "colab_type": "code",
        "outputId": "93c0f262-92dd-44ef-b8bd-a3ac06e0d243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 11s 1ms/step\n",
            "Test loss: 1.6708751831054687\n",
            "Test accuracy: 0.3873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UE3lF6EH1r_L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Save the trained weights in to .h5 format\n",
        "# model.save_weights(\"DNST_model.h5\")\n",
        "# print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ai-yZ2ED5AK1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# files.download('DNST_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Og56VCRh5j8V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0p2FLz6oJYWF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with open('/content/gdrive/My Drive/Model Weights/DNST_CIFAR10/foo.txt', 'w') as f:\n",
        "#   f.write('Hello Google Drive!')\n",
        "# !cat /content/gdrive/My\\ Drive/foo.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BVbBMYOrIRz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "TDl_LcL3bowG",
        "colab_type": "code",
        "outputId": "3fe68ad7-c964-47c0-9a03-5cde25ffc5bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10 \n",
        "def show_imgs(X):\n",
        "    pyplot.figure(1)\n",
        "    k = 0\n",
        "    for i in range(0,4):\n",
        "        for j in range(0,4):\n",
        "            pyplot.subplot2grid((4,4),(i,j))\n",
        "            pyplot.imshow(toimage(X[k]))\n",
        "            k = k+1\n",
        "    # show the plot\n",
        "    pyplot.show()\n",
        " \n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "show_imgs(x_test[:16])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a30e32e58e82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-a30e32e58e82>\u001b[0m in \u001b[0;36mshow_imgs\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pyplot' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "l9hCGSXHbqqI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}