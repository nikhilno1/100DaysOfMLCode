---

---

# Assignment 4 - Nikhil Utane  

## (EIP2-Batch 3)

------

There are two submissions for this assignment. This is done because different techniques were used and it wasn't clear if any of those violate the rules of the assignment.

Please click on the submission to open the corresponding notebook. Logs from multiple runs are consolidated into each notebook to make it easy to quickly go through the entire training logs. Appropriate headings have been added.

|                          Submission                          | Epochs for max acc. | Max val_acc |                      Comments                      |
| :----------------------------------------------------------: | :-----------------: | :---------: | :------------------------------------------------: |
| [Submission 1](https://github.com/nikhilno1/100DaysOfMLCode/blob/master/Submission%201_92.18%25.ipynb) |         133         |   92.18%    | For epochs 111 to 133, steps_per_epoch was doubled |
| [Submission 2](https://github.com/nikhilno1/100DaysOfMLCode/blob/master/Submission%202_92.44%25.ipynb) |         68          |   92.44%    | For epochs 51 to 68, data augmentation was removed |

Apart from what is mentioned in the comments above, many other things were tried. The most important of these were:

1. **<u>Learning Rate:</u>** Learning rate was the key parameter. Many different functions were tried. These were:
   - Cyclical Learning Rate
   - LR Reducer
   - Step Decay
   - LR Scheduler.
   - Manual setting after using LR finder

2. **<u>Batch size</u>**: Batch sizes were varied from 32 to 256. Little difference was found. Though batch size 64 seemed to give slightly higher accuracy.

3. **<u>Compression</u>**: Different compression values from 0.4 to 0.9 were used, though this did not seem to affect the accuracy much.

4. **<u>Dropout</u>**: Different dropout values were tried from 0.0 to 0.5. Did not notice significant difference but dropout 0.0 seemed to work better.

5. **<u>Number of layers</u>**: Number of layers within each block were varied from 12 to 15. Again did not observe much difference in accuracy.
6. **<u>Number of Blocks</u>**: Number of blocks were changed from 3 to 4. Did not observe much difference in accuracy.

7. **<u>Input Data Normalization</u>**: Different techniques of input data normalization were tried including z-score but a simple ' divide by 255' gave best results.
8. **<u>Image Augmentation</u>**: Various other options were tried. Rotation, shifting and horizontal flip gave best results.
9. **<u>SGD Initialization</u>**: Different settings were tried for momentum, decay and nesterov. Did not observe much difference in accuracy.

