{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5905,
     "status": "ok",
     "timestamp": 1540003949297,
     "user": {
      "displayName": "Nikhil Utane",
      "photoUrl": "",
      "userId": "17106433286041284792"
     },
     "user_tz": -330
    },
    "id": "K70hAckqg0EA",
    "outputId": "053fda5f-deb3-44e8-a715-643921e9217a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/\n",
    "!pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.misc import toimage\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the available GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Cyclical Learning Rate package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8729,
     "status": "ok",
     "timestamp": 1540003962783,
     "user": {
      "displayName": "Nikhil Utane",
      "photoUrl": "",
      "userId": "17106433286041284792"
     },
     "user_tz": -330
    },
    "id": "Mp9ytfaZ_N6P",
    "outputId": "29ee7921-52e2-48a6-dcd4-eb7e5fb192ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-20 02:47:20--  https://github.com/bckenstler/CLR/raw/master/clr_callback.py\n",
      "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/bckenstler/CLR/master/clr_callback.py [following]\n",
      "--2018-10-20 02:47:20--  https://raw.githubusercontent.com/bckenstler/CLR/master/clr_callback.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5326 (5.2K) [text/plain]\n",
      "Saving to: ‘clr_callback.py’\n",
      "\n",
      "\r",
      "clr_callback.py       0%[                    ]       0  --.-KB/s               \r",
      "clr_callback.py     100%[===================>]   5.20K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-10-20 02:47:21 (40.4 MB/s) - ‘clr_callback.py’ saved [5326/5326]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -rf clr_callback.py*\n",
    "!wget https://github.com/bckenstler/CLR/raw/master/clr_callback.py\n",
    "from clr_callback import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download LR Finder package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "Mp9ytfaZ_N6P",
    "outputId": "b31a7592-707c-43da-d11b-62c99776aca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-25 14:01:57--  https://gist.github.com/jeremyjordan/ac0229abd4b2b7000aca1643e88e0f02/raw/3b780822389cb4d96c50e83b59b8a506120fb044/lr_finder.py\n",
      "Resolving gist.github.com (gist.github.com)... 192.30.253.118, 192.30.253.119\n",
      "Connecting to gist.github.com (gist.github.com)|192.30.253.118|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://gist.githubusercontent.com/jeremyjordan/ac0229abd4b2b7000aca1643e88e0f02/raw/3b780822389cb4d96c50e83b59b8a506120fb044/lr_finder.py [following]\n",
      "--2018-10-25 14:01:58--  https://gist.githubusercontent.com/jeremyjordan/ac0229abd4b2b7000aca1643e88e0f02/raw/3b780822389cb4d96c50e83b59b8a506120fb044/lr_finder.py\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2847 (2.8K) [text/plain]\n",
      "Saving to: ‘lr_finder.py’\n",
      "\n",
      "\r",
      "lr_finder.py          0%[                    ]       0  --.-KB/s               \r",
      "lr_finder.py        100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-10-25 14:01:58 (46.9 MB/s) - ‘lr_finder.py’ saved [2847/2847]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /content/lr_finder.py*\n",
    "!wget https://gist.github.com/jeremyjordan/ac0229abd4b2b7000aca1643e88e0f02/raw/3b780822389cb4d96c50e83b59b8a506120fb044/lr_finder.py\n",
    "!sed -i '1s/^/from keras import backend as K\\n/' lr_finder.py\n",
    "from lr_finder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngfZDqIn169P"
   },
   "outputs": [],
   "source": [
    "def plot_model_history(model_history):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    # summarize history for accuracy\n",
    "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
    "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    # summarize history for loss\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3UF_GpLv18dK"
   },
   "outputs": [],
   "source": [
    "def accuracy(test_x, test_y, model):\n",
    "    result = model.predict(test_x)\n",
    "    predicted_class = np.argmax(result, axis=1)\n",
    "    true_class = np.argmax(test_y, axis=1)\n",
    "    num_correct = np.sum(predicted_class == true_class) \n",
    "    accuracy = float(num_correct)/result.shape[0]\n",
    "    return (accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "l = 12\n",
    "nb_filter = 24\n",
    "compression = 0.9\n",
    "dropout_rate = 0.2\n",
    "growth_rate = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Schedule Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Md0-RFxigndF"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    #lrate = 0.001\n",
    "    lrate = 0.01\n",
    "    if epoch > 75:\n",
    "        #lrate = 0.0005\n",
    "        lrate = 0.001\n",
    "    elif epoch > 150:\n",
    "        #lrate = 0.0003\n",
    "        lrate = 0.0001\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Step Decay function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IqCYykoAaeSE"
   },
   "outputs": [],
   "source": [
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "\tinitial_lrate = 0.1\n",
    "\tdrop = 0.5\n",
    "\tepochs_drop = 20.0\n",
    "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "\treturn lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21721,
     "status": "ok",
     "timestamp": 1540003993470,
     "user": {
      "displayName": "Nikhil Utane",
      "photoUrl": "",
      "userId": "17106433286041284792"
     },
     "user_tz": -330
    },
    "id": "mB7o3zu1g6eT",
    "outputId": "c78aefa9-c5b8-49e1-fea6-bbb9a82ecc4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 16s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoding \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Densenet Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression, growth_rate, nb_filter\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = Conv2D(growth_rate, (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        nb_filter += growth_rate\n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOP6IPsGhBwb"
   },
   "outputs": [],
   "source": [
    "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_filter * compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RaKFpubhDIC"
   },
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    #AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    GlobalAvg = GlobalAveragePooling2D()(relu)\n",
    "    #flat = Flatten()(AvgPooling)\n",
    "    output = Dense(num_classes, activation='softmax')(GlobalAvg)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Densenet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "#num_filter = 24\n",
    "#dropout_rate = 0.2\n",
    "#l = 12\n",
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = Conv2D(nb_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = add_denseblock(First_Conv2D, nb_filter, dropout_rate)\n",
    "First_Transition = add_transition(First_Block, nb_filter, dropout_rate)\n",
    "\n",
    "Second_Block = add_denseblock(First_Transition, nb_filter, dropout_rate)\n",
    "Second_Transition = add_transition(Second_Block, nb_filter, dropout_rate)\n",
    "\n",
    "Third_Block = add_denseblock(Second_Transition, nb_filter, dropout_rate)\n",
    "#Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "output = output_layer(Third_Block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7750
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1431,
     "status": "ok",
     "timestamp": 1540004007607,
     "user": {
      "displayName": "Nikhil Utane",
      "photoUrl": "",
      "userId": "17106433286041284792"
     },
     "user_tz": -330
    },
    "id": "1kFh7pdxhNtT",
    "outputId": "2ffe734d-e0ac-4b3a-fc0f-123fd80bff75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 24)   648         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 24)   96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 24)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 12)   2592        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 12)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 36)   0           conv2d_1[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 36)   144         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 36)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 12)   3888        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 12)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 48)   0           concatenate_1[0][0]              \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 48)   192         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 48)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 12)   5184        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 12)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 60)   0           concatenate_2[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 60)   240         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 60)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 12)   6480        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 12)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 72)   0           concatenate_3[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 72)   288         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 72)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 12)   7776        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 12)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 84)   0           concatenate_4[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 84)   336         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 84)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 12)   9072        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 12)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 96)   0           concatenate_5[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 96)   384         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 96)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 12)   10368       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 12)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 108)  0           concatenate_6[0][0]              \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 108)  432         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 108)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 12)   11664       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 32, 12)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 120)  0           concatenate_7[0][0]              \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 120)  480         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 120)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 12)   12960       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32, 32, 12)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 132)  0           concatenate_8[0][0]              \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 132)  528         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 132)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 12)   14256       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32, 32, 12)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 144)  0           concatenate_9[0][0]              \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 144)  576         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 144)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 12)   15552       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32, 32, 12)   0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 156)  0           concatenate_10[0][0]             \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 156)  624         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 156)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 12)   16848       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 12)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 168)  0           concatenate_11[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 168)  672         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 168)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 151)  25368       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 32, 32, 151)  0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 151)  0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 151)  604         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 151)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 12)   16308       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 16, 16, 12)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 163)  0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 163)  652         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 163)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 12)   17604       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 16, 16, 12)   0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 175)  0           concatenate_13[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 175)  700         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 175)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 12)   18900       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 16, 12)   0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 187)  0           concatenate_14[0][0]             \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 187)  748         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 187)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 12)   20196       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 16, 12)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 199)  0           concatenate_15[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 199)  796         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 199)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 12)   21492       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 16, 12)   0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 211)  0           concatenate_16[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 211)  844         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 211)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 12)   22788       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 16, 16, 12)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 223)  0           concatenate_17[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 223)  892         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 223)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 12)   24084       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 16, 16, 12)   0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 235)  0           concatenate_18[0][0]             \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 235)  940         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 235)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 12)   25380       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 16, 16, 12)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 247)  0           concatenate_19[0][0]             \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 247)  988         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 247)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 12)   26676       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 16, 16, 12)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 16, 16, 259)  0           concatenate_20[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 259)  1036        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 259)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 12)   27972       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 16, 16, 12)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 16, 16, 271)  0           concatenate_21[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 271)  1084        concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 271)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 12)   29268       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 16, 16, 12)   0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 16, 16, 283)  0           concatenate_22[0][0]             \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 283)  1132        concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 283)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 12)   30564       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 16, 16, 12)   0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 16, 16, 295)  0           concatenate_23[0][0]             \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 295)  1180        concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 295)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 280)  82600       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 16, 16, 280)  0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 8, 8, 280)    0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 280)    1120        average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 280)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 12)     30240       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 8, 8, 12)     0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 292)    0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 292)    1168        concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 292)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 12)     31536       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 8, 8, 12)     0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 8, 8, 304)    0           concatenate_25[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 304)    1216        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 304)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 12)     32832       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 8, 8, 12)     0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 8, 8, 316)    0           concatenate_26[0][0]             \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 316)    1264        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 316)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 12)     34128       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 8, 8, 12)     0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 8, 8, 328)    0           concatenate_27[0][0]             \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 328)    1312        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 328)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 12)     35424       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 8, 8, 12)     0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 8, 8, 340)    0           concatenate_28[0][0]             \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 340)    1360        concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 340)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 12)     36720       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 8, 8, 12)     0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 8, 8, 352)    0           concatenate_29[0][0]             \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 352)    1408        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 352)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 12)     38016       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 8, 8, 12)     0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 8, 8, 364)    0           concatenate_30[0][0]             \n",
      "                                                                 dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 364)    1456        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 364)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 12)     39312       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 8, 8, 12)     0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 8, 8, 376)    0           concatenate_31[0][0]             \n",
      "                                                                 dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 376)    1504        concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 376)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 12)     40608       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 8, 8, 12)     0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 388)    0           concatenate_32[0][0]             \n",
      "                                                                 dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 388)    1552        concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 388)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 12)     41904       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 8, 8, 12)     0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 400)    0           concatenate_33[0][0]             \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 400)    1600        concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 400)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 12)     43200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 8, 8, 12)     0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 412)    0           concatenate_34[0][0]             \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 412)    1648        concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 412)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 12)     44496       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 8, 8, 12)     0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 8, 8, 424)    0           concatenate_35[0][0]             \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 424)    1696        concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 424)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 424)          0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           4250        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 994,046\n",
      "Trainable params: 976,600\n",
      "Non-trainable params: 17,446\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive for saving and loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33413,
     "status": "ok",
     "timestamp": 1540004041350,
     "user": {
      "displayName": "Nikhil Utane",
      "photoUrl": "",
      "userId": "17106433286041284792"
     },
     "user_tz": -330
    },
    "id": "bZ5xJwV5L-rq",
    "outputId": "ce0ec144-b694-499a-b1ef-6644c025eefa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "#model.load_weights(\"/content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_aug.best.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "How_jcUCWhlC"
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NE92rHJDNsv8"
   },
   "outputs": [],
   "source": [
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define checkpoint and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBCgLZtupeJm"
   },
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath=\"/content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#clr_triangular = CyclicLR(mode='triangular2')\n",
    "#clr_triangular = CyclicLR(base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular2')\n",
    "#callbacks_list = [checkpoint]\n",
    "callbacks_list = [checkpoint, LearningRateScheduler(step_decay)]\n",
    "#callbacks_list = [checkpoint, clr_triangular]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "fnwj6hrDexZG",
    "outputId": "823f2e0d-0e1e-4d5a-d5a8-d9d0d004beb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 341s 7ms/step - loss: 0.6537 - acc: 0.7804\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+f/8avQ5iP/aPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96f51b0cc0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder = LRFinder(min_lr=1e-6,\n",
    "                     max_lr=1e-2, \n",
    "                     steps_per_epoch=np.ceil(epochs/batch_size), \n",
    "                     epochs=3)\n",
    "model.fit(x_train, y_train, callbacks=[lr_finder])\n",
    "lr_finder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1oTp_jR09VG"
   },
   "source": [
    "## Training - Epochs 1 to 50 (max val_acc: 89.90%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3572
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14146774,
     "status": "ok",
     "timestamp": 1540018201457,
     "user": {
      "displayName": "Nikhil Utane",
      "photoUrl": "",
      "userId": "17106433286041284792"
     },
     "user_tz": -330
    },
    "id": "l4_dcujlA56p",
    "outputId": "865cc183-d9da-42d2-d346-649d41473cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "781/781 [==============================] - 297s 380ms/step - loss: 1.6295 - acc: 0.4026 - val_loss: 2.2921 - val_acc: 0.3835\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.38350, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 2/50\n",
      "781/781 [==============================] - 289s 370ms/step - loss: 1.1709 - acc: 0.5828 - val_loss: 1.3741 - val_acc: 0.5795\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.38350 to 0.57950, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 3/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.9801 - acc: 0.6516 - val_loss: 1.2124 - val_acc: 0.6279\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.57950 to 0.62790, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 4/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.8449 - acc: 0.7012 - val_loss: 0.8596 - val_acc: 0.7194\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.62790 to 0.71940, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 5/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.7483 - acc: 0.7359 - val_loss: 1.8509 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.71940\n",
      "Epoch 6/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.6879 - acc: 0.7596 - val_loss: 0.8326 - val_acc: 0.7431\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.71940 to 0.74310, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 7/50\n",
      "781/781 [==============================] - 282s 362ms/step - loss: 0.6296 - acc: 0.7816 - val_loss: 0.7926 - val_acc: 0.7484\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.74310 to 0.74840, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 8/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.5901 - acc: 0.7944 - val_loss: 1.1013 - val_acc: 0.6754\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.74840\n",
      "Epoch 9/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.5593 - acc: 0.8061 - val_loss: 0.9367 - val_acc: 0.7262\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.74840\n",
      "Epoch 10/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.5338 - acc: 0.8130 - val_loss: 0.7929 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.74840 to 0.75170, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 11/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.5039 - acc: 0.8239 - val_loss: 0.5631 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.75170 to 0.81190, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 12/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.4831 - acc: 0.8330 - val_loss: 0.8763 - val_acc: 0.7551\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.81190\n",
      "Epoch 13/50\n",
      "781/781 [==============================] - 281s 360ms/step - loss: 0.4664 - acc: 0.8356 - val_loss: 0.6374 - val_acc: 0.8030\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.81190\n",
      "Epoch 14/50\n",
      "781/781 [==============================] - 281s 360ms/step - loss: 0.4512 - acc: 0.8444 - val_loss: 0.6417 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.81190\n",
      "Epoch 15/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.4264 - acc: 0.8521 - val_loss: 0.7126 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.81190\n",
      "Epoch 16/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.4179 - acc: 0.8527 - val_loss: 0.6880 - val_acc: 0.7880\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.81190\n",
      "Epoch 17/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.4016 - acc: 0.8606 - val_loss: 0.6057 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.81190 to 0.82370, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 18/50\n",
      "781/781 [==============================] - 281s 360ms/step - loss: 0.3871 - acc: 0.8657 - val_loss: 0.7443 - val_acc: 0.7805\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.82370\n",
      "Epoch 19/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.3724 - acc: 0.8718 - val_loss: 0.6630 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.82370\n",
      "Epoch 20/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.3198 - acc: 0.8891 - val_loss: 0.5424 - val_acc: 0.8376\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.82370 to 0.83760, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 21/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.3057 - acc: 0.8948 - val_loss: 0.5226 - val_acc: 0.8432\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.83760 to 0.84320, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 22/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.3003 - acc: 0.8936 - val_loss: 0.5107 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.84320 to 0.84870, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 23/50\n",
      "781/781 [==============================] - 281s 360ms/step - loss: 0.2961 - acc: 0.8978 - val_loss: 0.5115 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.84870 to 0.85410, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 24/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2886 - acc: 0.8980 - val_loss: 0.4853 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.85410 to 0.86080, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 25/50\n",
      "781/781 [==============================] - 281s 360ms/step - loss: 0.2855 - acc: 0.8994 - val_loss: 0.4692 - val_acc: 0.8624\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.86080 to 0.86240, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 26/50\n",
      "781/781 [==============================] - 281s 360ms/step - loss: 0.2784 - acc: 0.9022 - val_loss: 0.3958 - val_acc: 0.8779\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.86240 to 0.87790, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 27/50\n",
      "781/781 [==============================] - 282s 362ms/step - loss: 0.2721 - acc: 0.9039 - val_loss: 0.5529 - val_acc: 0.8457\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87790\n",
      "Epoch 28/50\n",
      "781/781 [==============================] - 282s 362ms/step - loss: 0.2706 - acc: 0.9058 - val_loss: 0.5278 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87790\n",
      "Epoch 29/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2626 - acc: 0.9077 - val_loss: 0.4310 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87790\n",
      "Epoch 30/50\n",
      "781/781 [==============================] - 282s 362ms/step - loss: 0.2553 - acc: 0.9097 - val_loss: 0.4716 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87790\n",
      "Epoch 31/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2576 - acc: 0.9104 - val_loss: 0.4161 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87790\n",
      "Epoch 32/50\n",
      "781/781 [==============================] - 281s 360ms/step - loss: 0.2530 - acc: 0.9126 - val_loss: 0.5410 - val_acc: 0.8494\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87790\n",
      "Epoch 33/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2425 - acc: 0.9136 - val_loss: 0.4264 - val_acc: 0.8768\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87790\n",
      "Epoch 34/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.2462 - acc: 0.9132 - val_loss: 0.4435 - val_acc: 0.8698\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87790\n",
      "Epoch 35/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2417 - acc: 0.9142 - val_loss: 0.4851 - val_acc: 0.8586\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87790\n",
      "Epoch 36/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2385 - acc: 0.9163 - val_loss: 0.5283 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87790\n",
      "Epoch 37/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2338 - acc: 0.9176 - val_loss: 0.4026 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.87790 to 0.88380, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 38/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2288 - acc: 0.9195 - val_loss: 0.5263 - val_acc: 0.8569\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88380\n",
      "Epoch 39/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2281 - acc: 0.9184 - val_loss: 0.4219 - val_acc: 0.8754\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88380\n",
      "Epoch 40/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.1978 - acc: 0.9322 - val_loss: 0.4542 - val_acc: 0.8723\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88380\n",
      "Epoch 41/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.1916 - acc: 0.9326 - val_loss: 0.4509 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88380\n",
      "Epoch 42/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.1866 - acc: 0.9345 - val_loss: 0.4885 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88380\n",
      "Epoch 43/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.1815 - acc: 0.9351 - val_loss: 0.4249 - val_acc: 0.8801\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88380\n",
      "Epoch 44/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.1833 - acc: 0.9359 - val_loss: 0.4012 - val_acc: 0.8863\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.88380 to 0.88630, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 45/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.1781 - acc: 0.9365 - val_loss: 0.4288 - val_acc: 0.8826\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.88630\n",
      "Epoch 46/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.1772 - acc: 0.9374 - val_loss: 0.4591 - val_acc: 0.8737\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.88630\n",
      "Epoch 47/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1773 - acc: 0.9376 - val_loss: 0.4003 - val_acc: 0.8912\n",
      "\n",
      "Epoch 00047: val_acc improved from 0.88630 to 0.89120, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 48/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.1779 - acc: 0.9373 - val_loss: 0.3783 - val_acc: 0.8919\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.89120 to 0.89190, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n",
      "Epoch 49/50\n",
      "781/781 [==============================] - 282s 360ms/step - loss: 0.1679 - acc: 0.9407 - val_loss: 0.4355 - val_acc: 0.8827\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89190\n",
      "Epoch 50/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.1680 - acc: 0.9406 - val_loss: 0.3504 - val_acc: 0.8990\n",
      "\n",
      "Epoch 00050: val_acc improved from 0.89190 to 0.89900, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_with_lrsch_dropout.best.h5\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\\\n",
    "                    verbose=1, validation_data=(x_test,y_test),\\\n",
    "                    callbacks=callbacks_list) \n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1oTp_jR09VG"
   },
   "source": [
    "## Training - Epochs 51 to 100 (max val_acc: 91.33%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3519
    },
    "colab_type": "code",
    "id": "l4_dcujlA56p",
    "outputId": "6010adcd-fec1-476b-cfa5-2b78778690d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.2897 - acc: 0.8971 - val_loss: 0.5847 - val_acc: 0.8407\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.84070, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 2/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2675 - acc: 0.9055 - val_loss: 0.5162 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.84070 to 0.85470, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 3/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2661 - acc: 0.9054 - val_loss: 0.5573 - val_acc: 0.8482\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.85470\n",
      "Epoch 4/50\n",
      "781/781 [==============================] - 282s 362ms/step - loss: 0.2535 - acc: 0.9112 - val_loss: 0.4239 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.85470 to 0.87500, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 5/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.2545 - acc: 0.9104 - val_loss: 0.4956 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.87500\n",
      "Epoch 6/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2483 - acc: 0.9117 - val_loss: 0.6446 - val_acc: 0.8183\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.87500\n",
      "Epoch 7/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2458 - acc: 0.9138 - val_loss: 0.8425 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.87500\n",
      "Epoch 8/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2351 - acc: 0.9170 - val_loss: 0.4691 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.87500\n",
      "Epoch 9/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2301 - acc: 0.9190 - val_loss: 0.4160 - val_acc: 0.8805\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.87500 to 0.88050, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 10/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.2309 - acc: 0.9169 - val_loss: 0.5236 - val_acc: 0.8516\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88050\n",
      "Epoch 11/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2227 - acc: 0.9206 - val_loss: 0.4398 - val_acc: 0.8678\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88050\n",
      "Epoch 12/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.2184 - acc: 0.9215 - val_loss: 0.4017 - val_acc: 0.8851\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.88050 to 0.88510, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 13/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.2118 - acc: 0.9244 - val_loss: 0.4024 - val_acc: 0.8828\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88510\n",
      "Epoch 14/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.2087 - acc: 0.9256 - val_loss: 0.4782 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.88510\n",
      "Epoch 15/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.2087 - acc: 0.9264 - val_loss: 0.4859 - val_acc: 0.8638\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88510\n",
      "Epoch 16/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.2043 - acc: 0.9296 - val_loss: 0.4457 - val_acc: 0.8791\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.88510\n",
      "Epoch 17/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.2024 - acc: 0.9274 - val_loss: 0.5673 - val_acc: 0.8559\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.88510\n",
      "Epoch 18/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.2002 - acc: 0.9282 - val_loss: 0.3970 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.88510 to 0.88950, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 19/50\n",
      "781/781 [==============================] - 282s 362ms/step - loss: 0.1937 - acc: 0.9317 - val_loss: 0.5637 - val_acc: 0.8556\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.88950\n",
      "Epoch 20/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1544 - acc: 0.9452 - val_loss: 0.3779 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.88950 to 0.89420, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 21/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1452 - acc: 0.9488 - val_loss: 0.3833 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.89420 to 0.89490, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 22/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1430 - acc: 0.9491 - val_loss: 0.3463 - val_acc: 0.9017\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.89490 to 0.90170, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 23/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1399 - acc: 0.9515 - val_loss: 0.3890 - val_acc: 0.8941\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.90170\n",
      "Epoch 24/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1407 - acc: 0.9493 - val_loss: 0.3740 - val_acc: 0.8983\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.90170\n",
      "Epoch 25/50\n",
      "781/781 [==============================] - 283s 363ms/step - loss: 0.1340 - acc: 0.9519 - val_loss: 0.3947 - val_acc: 0.8921\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.90170\n",
      "Epoch 26/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1317 - acc: 0.9523 - val_loss: 0.3717 - val_acc: 0.8992\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.90170\n",
      "Epoch 27/50\n",
      "781/781 [==============================] - 283s 363ms/step - loss: 0.1343 - acc: 0.9523 - val_loss: 0.4342 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.90170\n",
      "Epoch 28/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1304 - acc: 0.9539 - val_loss: 0.3890 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.90170 to 0.90280, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 29/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1289 - acc: 0.9551 - val_loss: 0.4423 - val_acc: 0.8897\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.90280\n",
      "Epoch 30/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1274 - acc: 0.9542 - val_loss: 0.3703 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.90280\n",
      "Epoch 31/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1244 - acc: 0.9555 - val_loss: 0.4097 - val_acc: 0.8984\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.90280\n",
      "Epoch 32/50\n",
      "781/781 [==============================] - 282s 362ms/step - loss: 0.1238 - acc: 0.9559 - val_loss: 0.4199 - val_acc: 0.8952\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.90280\n",
      "Epoch 33/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1257 - acc: 0.9549 - val_loss: 0.4147 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.90280\n",
      "Epoch 34/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1217 - acc: 0.9564 - val_loss: 0.4064 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.90280\n",
      "Epoch 35/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1191 - acc: 0.9584 - val_loss: 0.4056 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.90280\n",
      "Epoch 36/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1261 - acc: 0.9551 - val_loss: 0.3701 - val_acc: 0.9064\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.90280 to 0.90640, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 37/50\n",
      "781/781 [==============================] - 282s 362ms/step - loss: 0.1200 - acc: 0.9568 - val_loss: 0.3878 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.90640\n",
      "Epoch 38/50\n",
      "781/781 [==============================] - 282s 362ms/step - loss: 0.1189 - acc: 0.9577 - val_loss: 0.4136 - val_acc: 0.9004\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.90640\n",
      "Epoch 39/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1179 - acc: 0.9582 - val_loss: 0.4779 - val_acc: 0.8870\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.90640\n",
      "Epoch 40/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.1009 - acc: 0.9636 - val_loss: 0.3992 - val_acc: 0.9041\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.90640\n",
      "Epoch 41/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.0921 - acc: 0.9668 - val_loss: 0.3706 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.90640 to 0.90770, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 42/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0933 - acc: 0.9678 - val_loss: 0.3496 - val_acc: 0.9133\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.90770 to 0.91330, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 43/50\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.0910 - acc: 0.9678 - val_loss: 0.3844 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.91330\n",
      "Epoch 44/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0894 - acc: 0.9686 - val_loss: 0.3703 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.91330\n",
      "Epoch 45/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0906 - acc: 0.9679 - val_loss: 0.4008 - val_acc: 0.9030\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.91330\n",
      "Epoch 46/50\n",
      "781/781 [==============================] - 282s 362ms/step - loss: 0.0882 - acc: 0.9691 - val_loss: 0.3764 - val_acc: 0.9101\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.91330\n",
      "Epoch 47/50\n",
      "781/781 [==============================] - 282s 362ms/step - loss: 0.0850 - acc: 0.9703 - val_loss: 0.3691 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.91330\n",
      "Epoch 48/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0884 - acc: 0.9678 - val_loss: 0.3909 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.91330\n",
      "Epoch 49/50\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0826 - acc: 0.9706 - val_loss: 0.4125 - val_acc: 0.9046\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.91330\n",
      "Epoch 50/50\n",
      "  1/781 [..............................] - ETA: 4:26 - loss: 0.1133 - acc: 0.9688"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\\\n",
    "                    verbose=1, validation_data=(x_test,y_test),\\\n",
    "                    callbacks=callbacks_list) \n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1oTp_jR09VG"
   },
   "source": [
    "## Training - Epochs 101 to 110 (max val_acc: 91.68%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "colab_type": "code",
    "id": "l4_dcujlA56p",
    "outputId": "dc61a4bf-23b8-484c-e955-042882e43424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "781/781 [==============================] - 291s 373ms/step - loss: 0.0530 - acc: 0.9810 - val_loss: 0.3768 - val_acc: 0.9159\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91590, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 283s 362ms/step - loss: 0.0529 - acc: 0.9817 - val_loss: 0.3738 - val_acc: 0.9161\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91590 to 0.91610, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0534 - acc: 0.9811 - val_loss: 0.3739 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.91610 to 0.91660, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0534 - acc: 0.9808 - val_loss: 0.3757 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.91660\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0525 - acc: 0.9820 - val_loss: 0.3754 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.91660\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0526 - acc: 0.9812 - val_loss: 0.3727 - val_acc: 0.9168\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.91660 to 0.91680, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0509 - acc: 0.9821 - val_loss: 0.3733 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.91680\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0511 - acc: 0.9823 - val_loss: 0.3781 - val_acc: 0.9159\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.91680\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 282s 361ms/step - loss: 0.0528 - acc: 0.9821 - val_loss: 0.3740 - val_acc: 0.9162\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91680\n",
      "Epoch 10/10\n",
      "  1/781 [..............................] - ETA: 4:24 - loss: 0.0463 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\\\n",
    "                    verbose=1, validation_data=(x_test,y_test),\\\n",
    "                    callbacks=callbacks_list) \n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1oTp_jR09VG"
   },
   "source": [
    "## Training - Epochs 111 to 139 (max val_acc: 92.18% FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2115
    },
    "colab_type": "code",
    "id": "l4_dcujlA56p",
    "outputId": "b6010fee-2061-4bc1-dc84-409461d26fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 517s 331ms/step - loss: 0.0259 - acc: 0.9907 - val_loss: 0.4067 - val_acc: 0.9155\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91550, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 501s 321ms/step - loss: 0.0207 - acc: 0.9928 - val_loss: 0.3973 - val_acc: 0.9172\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.91550 to 0.91720, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 501s 321ms/step - loss: 0.0196 - acc: 0.9934 - val_loss: 0.3908 - val_acc: 0.9172\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.91720\n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 501s 320ms/step - loss: 0.0182 - acc: 0.9937 - val_loss: 0.3923 - val_acc: 0.9171\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.91720\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 501s 321ms/step - loss: 0.0181 - acc: 0.9938 - val_loss: 0.3903 - val_acc: 0.9172\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.91720\n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 501s 321ms/step - loss: 0.0168 - acc: 0.9944 - val_loss: 0.3866 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.91720 to 0.91830, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 502s 321ms/step - loss: 0.0169 - acc: 0.9943 - val_loss: 0.3848 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.91830 to 0.91890, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 8/100\n",
      "1562/1562 [==============================] - 502s 322ms/step - loss: 0.0164 - acc: 0.9944 - val_loss: 0.3842 - val_acc: 0.9196\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.91890 to 0.91960, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 9/100\n",
      "1562/1562 [==============================] - 502s 321ms/step - loss: 0.0161 - acc: 0.9946 - val_loss: 0.3810 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91960\n",
      "Epoch 10/100\n",
      "1562/1562 [==============================] - 505s 323ms/step - loss: 0.0151 - acc: 0.9950 - val_loss: 0.3774 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.91960 to 0.92020, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 11/100\n",
      "1562/1562 [==============================] - 504s 323ms/step - loss: 0.0153 - acc: 0.9948 - val_loss: 0.3796 - val_acc: 0.9207\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.92020 to 0.92070, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 12/100\n",
      "1562/1562 [==============================] - 505s 323ms/step - loss: 0.0158 - acc: 0.9947 - val_loss: 0.3808 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92070\n",
      "Epoch 13/100\n",
      "1562/1562 [==============================] - 504s 323ms/step - loss: 0.0144 - acc: 0.9953 - val_loss: 0.3781 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92070\n",
      "Epoch 14/100\n",
      "1562/1562 [==============================] - 504s 323ms/step - loss: 0.0145 - acc: 0.9952 - val_loss: 0.3798 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92070\n",
      "Epoch 15/100\n",
      "1562/1562 [==============================] - 504s 323ms/step - loss: 0.0140 - acc: 0.9953 - val_loss: 0.3819 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92070\n",
      "Epoch 16/100\n",
      "1562/1562 [==============================] - 504s 323ms/step - loss: 0.0146 - acc: 0.9952 - val_loss: 0.3786 - val_acc: 0.9206\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92070\n",
      "Epoch 17/100\n",
      "1562/1562 [==============================] - 504s 322ms/step - loss: 0.0137 - acc: 0.9958 - val_loss: 0.3789 - val_acc: 0.9201\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92070\n",
      "Epoch 18/100\n",
      "1562/1562 [==============================] - 505s 323ms/step - loss: 0.0139 - acc: 0.9954 - val_loss: 0.3778 - val_acc: 0.9207\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92070\n",
      "Epoch 19/100\n",
      "1562/1562 [==============================] - 504s 323ms/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.3776 - val_acc: 0.9200\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92070\n",
      "Epoch 20/100\n",
      "1562/1562 [==============================] - 505s 323ms/step - loss: 0.0138 - acc: 0.9956 - val_loss: 0.3749 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.92070 to 0.92080, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 21/100\n",
      "1562/1562 [==============================] - 505s 323ms/step - loss: 0.0135 - acc: 0.9955 - val_loss: 0.3762 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.92080\n",
      "Epoch 22/100\n",
      "1562/1562 [==============================] - 505s 323ms/step - loss: 0.0138 - acc: 0.9954 - val_loss: 0.3757 - val_acc: 0.9217\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.92080 to 0.92170, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 23/100\n",
      "1562/1562 [==============================] - 505s 323ms/step - loss: 0.0134 - acc: 0.9955 - val_loss: 0.3758 - val_acc: 0.9218\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.92170 to 0.92180, saving model to /content/gdrive/My Drive/Model Weights/DNST_CIFAR10/weights_3B.best.h5\n",
      "Epoch 24/100\n",
      "1562/1562 [==============================] - 506s 324ms/step - loss: 0.0136 - acc: 0.9956 - val_loss: 0.3775 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.92180\n",
      "Epoch 25/100\n",
      "1562/1562 [==============================] - 506s 324ms/step - loss: 0.0129 - acc: 0.9957 - val_loss: 0.3756 - val_acc: 0.9206\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.92180\n",
      "Epoch 26/100\n",
      "1562/1562 [==============================] - 506s 324ms/step - loss: 0.0129 - acc: 0.9961 - val_loss: 0.3774 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.92180\n",
      "Epoch 27/100\n",
      "1562/1562 [==============================] - 506s 324ms/step - loss: 0.0134 - acc: 0.9955 - val_loss: 0.3785 - val_acc: 0.9199\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.92180\n",
      "Epoch 28/100\n",
      "1562/1562 [==============================] - 506s 324ms/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.3764 - val_acc: 0.9206\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.92180\n",
      "Epoch 29/100\n",
      "1562/1562 [==============================] - 504s 323ms/step - loss: 0.0121 - acc: 0.9963 - val_loss: 0.3777 - val_acc: 0.9214\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.92180\n",
      "Epoch 30/100\n",
      "1304/1562 [========================>.....] - ETA: 1:20 - loss: 0.0134 - acc: 0.9956"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model_info = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=2*x_train.shape[0] // batch_size,epochs=epochs,\\\n",
    "                    verbose=1, validation_data=(x_test,y_test),\\\n",
    "                    callbacks=callbacks_list) \n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Model History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22841,
     "status": "ok",
     "timestamp": 1540018225264,
     "user": {
      "displayName": "Nikhil Utane",
      "photoUrl": "",
      "userId": "17106433286041284792"
     },
     "user_tz": -330
    },
    "id": "yzvKjFGh9sog",
    "outputId": "132992a0-78aa-4708-eafa-11a7ee155862"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3YAAAFMCAYAAACH0y5vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7ERERERERNqcCjsREREREZE29/9C4AQ09IeGPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f175c590358>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model took 14145.63 seconds to train\n",
      "Accuracy on test data is: 89.90\n"
     ]
    }
   ],
   "source": [
    "# plot model history\n",
    "plot_model_history(model_info)\n",
    "print(\"Model took %0.2f seconds to train\" % (end - start))\n",
    "# compute test accuracy\n",
    "print(\"Accuracy on test data is: %0.2f\" % accuracy(x_test, y_test, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22123,
     "status": "ok",
     "timestamp": 1540018247621,
     "user": {
      "displayName": "Nikhil Utane",
      "photoUrl": "",
      "userId": "17106433286041284792"
     },
     "user_tz": -330
    },
    "id": "ZcWydmIVhZGr",
    "outputId": "19df5be1-0c06-4871-89d9-c93408f1c06a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 21s 2ms/step\n",
      "Test loss: 0.35043127034306526\n",
      "Test accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9hCGSXHbqqI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNST_CIFAR10.ipynb",
   "provenance": [
    {
     "file_id": "1pFn0wvWOKj93A4_-pjMxsxR96VDyN-NF",
     "timestamp": 1539753040190
    },
    {
     "file_id": "1_1kwmwgL7g94jI6BEtcgm-D2_AFk0zxK",
     "timestamp": 1519101209834
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
